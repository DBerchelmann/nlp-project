{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\"><img width=\"275\" height=\"50\" src=\"http://www.essentiallysports.com/wp-content/uploads/f1-logo.jpg\"/> </div> \n",
    "\n",
    "<div align=\"center\"> <h1>NLP Project</h1> \n",
    "  <h6> by David Berchelmann -- May 11, 2021 </h6> </div>\n",
    "  \n",
    "  ------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img width=\"800\" height=\"50\" src=\"https://communityimpact.com/wp-content/uploads/2016/10/cropCircuitoftheAmericas_USGP_20151023_Rizzo_89640-2.jpg\" /> </div>\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "<h1> Welcome! </h1>\n",
    "\n",
    "The following jupyter notebook will take you through my NLP project focusing on predicting the programming languages that make up git hub repositories that focus on Formula 1.  \n",
    "\n",
    "All of the files and notebooks for this project can be accessed via the github repostiory located at --> https://github.com/DBerchelmann/nlp-project\n",
    "\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Executive Summary </h1>\n",
    "\n",
    "------\n",
    "\n",
    "<h4><b>The Problem</b></h4>\n",
    "\n",
    "- Can a repository's programming language be determined by a model built off of natural language processing?\n",
    "\n",
    "<h4><b>The Goal</b></h4>\n",
    "\n",
    "- Create a predictive model using natural language processing that can determine the dominant programming language used in github repositories based on the text of the README files.\n",
    "\n",
    "<h4><b>The Process</b></h4>\n",
    "\n",
    "  * Acquire the Data\n",
    "  * Prepare \n",
    "  * Explore \n",
    "  * Model\n",
    "  * Findings/Recommendations\n",
    "  \n",
    "<h4><b>The Findings</b></h4>\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3><u>Environment Setup</u></h3>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from env import github_token, github_username\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h3><u>Acquire the Data</u></h3>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acquire section took several steps before it could be read into the notebook as a csv.\n",
    "\n",
    "- Use a function to search a pre-determined amount of pages based on a search for github repository URLs that had the highest star rankings which covered Formula 1.\n",
    "- Next,those URLs were inputted into another function which then used web scraping to bring back the repository name, the dominant language it was written in, and the text contents of the READEME file.\n",
    "- Finally, that list of dictionaries was converted into a pandas data frame which was then written into a CSV.\n",
    "- After being written into a CSV, I was able to read it back into this notebook as a CSV which will allow for much faster preparation, exploring, and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv (r'f1_readmes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 326 entries, 0 to 325\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   repo             326 non-null    object\n",
      " 1   language         296 non-null    object\n",
      " 2   readme_contents  268 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 7.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Takeaways</h3>\n",
    "\n",
    "- We have some nulls in README that will need to be investigated. NLP needs text to read to be able to predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h3><u>Prepare the Data</u></h3>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Initial Findings</h4>\n",
    "\n",
    "- I saw that while a majority of the readme files were in english, there were a number of them in different languages. \n",
    "- I utilized a language detection import called TextBlob that allowed me to decipher those languages.\n",
    "    - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
